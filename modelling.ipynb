{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","pd.set_option('display.max_columns', None)\n","pd.set_option(\"display.max_rows\", 25)\n","from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n","from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.feature_selection import RFECV\n","import seaborn as sns\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n","from sklearn import feature_selection\n","from sklearn import metrics\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","from featureModelling import get_features_with_cleaned_results"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["feature_df = get_features_with_cleaned_results()\n","feature_columns = [col for col in feature_df if col.startswith('feature_')]\n","\n","# Create our test and train sets\n","test_x_2019 = feature_df.loc[feature_df.season == 2019, ['game'] + feature_columns]\n","test_y_2019 = feature_df.loc[feature_df.season == 2019, 'result']\n","X = feature_df.loc[feature_df.season < 2018, ['game'] + feature_columns]\n","y = feature_df.loc[feature_df.season < 2018, 'result']\n","\n","\n","test_x_2018 = feature_df.loc[feature_df.season == 2018, ['game'] + feature_columns]\n","test_y_2018 = feature_df.loc[feature_df.season == 2018, 'result']\n","\n","# Scale features\n","scaler = StandardScaler()\n","X[feature_columns] = scaler.fit_transform(X[feature_columns])\n","test_x_2018[feature_columns] = scaler.transform(test_x_2018[feature_columns])\n","test_x_2019[feature_columns] = scaler.transform(test_x_2019[feature_columns])\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Our accuracy in predicting the 2019 season is: 58.94%\nOur accuracy in predicting the 2018 season is: 56.04%\nCPU times: user 98.1 ms, sys: 49.8 ms, total: 148 ms\nWall time: 73.3 ms\n"}],"source":["%%time\n","chosenAlgorithm = LogisticRegression()\n","chosenAlgorithm.fit(X, y)\n","\n","final_predictions = chosenAlgorithm.predict(test_x_2018)\n","final_predictions2 = chosenAlgorithm.predict(test_x_2019)\n","\n","accuracy2018 = (final_predictions == test_y_2018).mean() * 100\n","accuracy2019 = (final_predictions2 == test_y_2019).mean() * 100\n","print(\"Accuracy in predicting the 2019 season is: {:.2f}%\".format(accuracy2019))\n","print(\"Accuracy in predicting the 2018 season is: {:.2f}%\".format(accuracy2018))\n","\n","# print(confusion_matrix(test_y_2018,final_predictions))\n","# print(classification_report(test_y_2018,final_predictions))\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we want to tune our parameters.\n","\n","Interestingly, this is actually an unconstrained nonlinear optimization problem and will be solved with either newton or quasi-newton (L-BFGS) methods. As expected Newton's method is better (but slower) as the quasi-newton simply tries to approximate the jacobian."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"1min 26s ± 10.6 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"}],"source":["%%timeit\n","def hyperparameterTuning(X, y, algorithm, nfolds):\n","    kfold = StratifiedKFold(n_splits=nfolds)\n","    Cs = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5,  0.2, 1, 10]\n","    solvers = [\"newton-cg\", \"lbfgs\", \"liblinear\", \"bfgs\", \"sag\", \"saga\"]\n","    param_grid = {'C': Cs, 'solver': solvers}\n","    #param_grid = {'C' : Cs, 'solver' : solvers}\n","    grid_search = GridSearchCV(algorithm, param_grid, cv=kfold)\n","    grid_search.fit(X,y)\n","    grid_search.best_params_\n","    return grid_search\n","\n","optimal_parameters = hyperparameterTuning(X,y,chosenAlgorithm,5).best_params_"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Default parameters are: \n{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n\noptimal parameters are:\n{'C': 0.05, 'solver': 'newton-cg'}\n"}],"source":["print(\"Default parameters are: \")\n","print(chosenAlgorithm.get_params())\n","print(\"\\noptimal parameters are:\")\n","print(optimal_parameters)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Our accuracy in predicting the 2018 season is: 63.77%\n"}],"source":["TunedAlgorithm = LogisticRegression(**optimal_parameters)\n","TunedAlgorithm.fit(X,y)\n","tunedPredictions = TunedAlgorithm.predict(test_x_2018)\n","\n","tunedAccuracy = (tunedPredictions == test_y_2018).mean() * 100\n","\n","print(\"Tuned Accuracy in predicting the 2018 season is: {:.2f}%\".format(tunedAccuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.9-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python36964biteef8d8053aa54c489f0d9a087cd5ec38","display_name":"Python 3.6.9 64-bit"}}}